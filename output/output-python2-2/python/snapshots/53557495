from cctbx import maptbx
from cctbx import miller
from cctbx import crystal
from cctbx import sgtbx
from cctbx import adptbx
from libtbx import table_utils
import cctbx.sgtbx.lattice_symmetry
import cctbx.sgtbx.cosets
from cctbx.array_family import flex
from libtbx.utils import Sorry, date_and_time, multi_out
import iotbx.phil
from iotbx import reflection_file_reader
from iotbx import reflection_file_utils
from iotbx import crystal_symmetry_from_any
import mmtbx.scaling
from mmtbx.scaling import absolute_scaling
from mmtbx.scaling import matthews, twin_analyses
from mmtbx.scaling import basic_analyses, data_statistics
import libtbx.phil.command_line
from cStringIO import StringIO
from scitbx.python_utils import easy_pickle, robust_statistics
import sys, os, math
import scitbx.lbfgs



class reindexing(object):
  __doc__=""" Reindexing matrices """
  def __init__(self,
               set_a,
               set_b,
               out=None,
               relative_length_tolerance=0.05,
               absolute_angle_tolerance=3.0,
               lattice_symmetry_max_delta=3.0):

    self.relative_length_tolerance=relative_length_tolerance
    self.absolute_angle_tolerance=absolute_angle_tolerance
    self.lattice_symmetry_max_delta=lattice_symmetry_max_delta
    self.out = out
    if self.out is None:
      self.out = sys.stdout


    ## ameka deep copy as we,
    self.set_a = set_a.deep_copy().set_observation_type( set_a )
    self.set_b = set_b.deep_copy().set_observation_type( set_b )


    # We need to go to the minimum cell
    self.change_of_basis_op_to_minimum_cell_a=\
      self.set_a.change_of_basis_op_to_minimum_cell()
    self.change_of_basis_op_to_minimum_cell_b=\
      self.set_b.change_of_basis_op_to_minimum_cell()

    ## make the change
    self.set_a  = self.set_a.change_basis(
      self.change_of_basis_op_to_minimum_cell_a ).map_to_asu().\
       set_observation_type( set_a )

    self.set_b  = self.set_b.change_basis(
      self.change_of_basis_op_to_minimum_cell_b ).map_to_asu().\
       set_observation_type( set_b )

    ## Get the lattice group
    self.lattice_group_a = sgtbx.lattice_symmetry.group(
      self.set_a.unit_cell(),
      max_delta=self.lattice_symmetry_max_delta)
    self.lattice_group_b = sgtbx.lattice_symmetry.group(
      self.set_b.unit_cell(),
      max_delta=self.lattice_symmetry_max_delta)

    ## Get the lattice symmetry
    self.lattice_symmetry_a = crystal.symmetry(
      unit_cell=self.set_a.unit_cell(),
      space_group_info=sgtbx.space_group_info(group=self.lattice_group_a),
      assert_is_compatible_unit_cell=False)

    self.lattice_symmetry_b = crystal.symmetry(
      unit_cell=self.set_b.unit_cell(),
      space_group_info=sgtbx.space_group_info(group=self.lattice_group_b),
      assert_is_compatible_unit_cell=False)

    ## Get the intensity symmetry please
    tmp_a = crystal.symmetry.change_basis(
      self.set_a,
      self.set_a.change_of_basis_op_to_minimum_cell())
    tmp_b = crystal.symmetry.change_basis(
      self.set_b,
      self.set_b.change_of_basis_op_to_minimum_cell())

    self.intensity_symmetry_a = tmp_a.reflection_intensity_symmetry(
      anomalous_flag=self.set_a.anomalous_flag() )
    self.intensity_symmetry_b = tmp_b.reflection_intensity_symmetry(
      anomalous_flag=self.set_b.anomalous_flag() )


    self.intensity_symmetry_a = self.set_a.reflection_intensity_symmetry()
    self.intensity_symmetry_b = self.set_b.reflection_intensity_symmetry()


    c_inv_rs = self.set_a.unit_cell().\
      similarity_transformations(
        other=self.set_b.unit_cell(),
        relative_length_tolerance=self.relative_length_tolerance,
        absolute_angle_tolerance=self.absolute_angle_tolerance)

    min_bases_msd = None
    similarity_cb_op = None

    result = None

    for c_inv_r in c_inv_rs:
      c_inv = sgtbx.rt_mx(sgtbx.rot_mx(c_inv_r))
      cb_op = sgtbx.change_of_basis_op(c_inv).inverse()
      bases_msd = self.set_a.unit_cell() \
        .bases_mean_square_difference(
          other=cb_op.apply(self.set_b.unit_cell()))
      if ( (min_bases_msd is None) or (min_bases_msd > bases_msd) ):
        min_bases_msd = bases_msd
        similarity_cb_op = cb_op
    if (similarity_cb_op is None):
      result

    common_lattice_group = sgtbx.space_group(self.lattice_group_a)
    for s in self.lattice_group_b.build_derived_acentric_group() \
               .change_basis(similarity_cb_op):
      try: common_lattice_group.expand_smx(s)
      except RuntimeError:
        result = []

    common_lattice_group.make_tidy()
    result = []

    for s in sgtbx.cosets.double_unique(
               g=common_lattice_group,
               h1=self.intensity_symmetry_a.space_group()
                   .build_derived_acentric_group()
                   .make_tidy(),
               h2=self.intensity_symmetry_b.space_group()
                   .build_derived_acentric_group()
                   .change_basis(similarity_cb_op)
                   .make_tidy()):
      if (s.r().determinant() > 0):
        result.append(sgtbx.change_of_basis_op(s) * similarity_cb_op)
    self.matrices = result
    self.matrices_for_table = []
    self.cc_values= []
    self.matches = []
    self.table=None
    self.analyse()

  def combined_cb_op(self, cb_op):
    s = self.change_of_basis_op_to_minimum_cell_a
    o = self.change_of_basis_op_to_minimum_cell_b
    return s.inverse() * cb_op.new_denominators(s) * o

  def analyse(self):
    table_data=[]
    for cb_op in self.matrices:
      cb_op_comb = self.combined_cb_op(cb_op)
      self.matrices_for_table.append( cb_op_comb )

      tmp_set_b = self.set_b.change_basis(cb_op).map_to_asu()

      tmp_set_a, tmp_set_b = self.set_a.map_to_asu().common_sets(
        tmp_set_b,
        assert_is_similar_symmetry=False)
      tmp_cc = tmp_set_a.correlation(
        tmp_set_b,
        assert_is_similar_symmetry=False)
      ## STore the cc values
      self.cc_values.append(  tmp_cc.coefficient()  )
      self.matches.append(
        float(tmp_set_a.indices().size())/float(self.set_a.indices().size())
      )

  def select_and_transform(self,
                           matches_cut_off=0.75
                          ):
    ## hopsa
    max_cc=-1.0
    location = 0
    table_data=[]
    for ii in range(len(self.matrices)):
      table_data.append(
        [self.matrices_for_table[ii].as_hkl(),
         "%4.3f"%(self.cc_values[ii]),
         "%4.3f"%(self.matches[ii]),
         '   ']
        )

      if self.matches[ii]>=matches_cut_off:
        if max_cc<self.cc_values[ii]:
          max_cc = self.cc_values[ii]
          location = ii

    legend = ('Operator', 'Correlation', 'matches (%)', 'choice')
    table_data[location][3]=' <--- '
    self.table = table_utils.format([legend]+table_data,
                                       comments=None,
                                       has_header=True,
                                       separate_rows=False,
                                       prefix='| ',
                                       postfix=' |')

    print >> self.out, self.table
    ##  change things in primitive setting
    transform_b = self.set_b.change_basis( self.matrices[ location ] ).map_to_asu()
    ##  go back to standard setting
    transform_b = transform_b.change_basis(
      self.change_of_basis_op_to_minimum_cell_b.inverse() ).map_to_asu()\
      .set_observation_type( self.set_b )

    return ( transform_b )


class delta_generator(object):
  def __init__(self,
               nat,
               der,
               nsr_bias=1.0):
    self.nat=nat.deep_copy()
    self.der=der.deep_copy()
    self.nsr_bias=1.0/nsr_bias

    assert self.nat.is_real_array()
    assert self.nat.is_real_array()

    if self.nat.is_xray_intensity_array():
      self.nat.f_sq_as_f()
    if self.der.is_xray_intensity_array():
      self.der.f_sq_as_f()

    self.nat,self.der = self.nat.common_sets(self.der)

    self.der = self.der.customized_copy(
      data = self.der.data()*self.nsr_bias,
      sigmas = self.der.sigmas()*self.nsr_bias).set_observation_type(
        self.der)

    self.delta_f=self.nat.customized_copy(
      data = ( self.der.data() - self.nat.data() ),
      sigmas = flex.sqrt( self.der.sigmas()*self.der.sigmas()+
                          self.nat.sigmas()*self.nat.sigmas() )
      ).set_observation_type( self.nat )

    self.abs_delta_f=self.nat.customized_copy(
      data = flex.abs( self.der.data() - self.nat.data() ),
      sigmas = flex.sqrt( self.der.sigmas()*self.der.sigmas()+
                          self.nat.sigmas()*self.nat.sigmas() )
      ).set_observation_type( self.der )

    if not self.nat.is_xray_intensity_array():
      self.nat.f_as_f_sq()
    if not self.der.is_xray_intensity_array():
      self.der.f_as_f_sq()

    self.delta_i=self.nat.customized_copy(
      data = ( self.der.data() - self.nat.data() ),
      sigmas = flex.sqrt( self.der.sigmas()*self.der.sigmas()+
                          self.nat.sigmas()*self.nat.sigmas() )
      ).set_observation_type( self.nat )

    self.abs_delta_i=self.nat.customized_copy(
      data = flex.abs( self.der.data() - self.nat.data() ),
      sigmas = flex.sqrt( self.der.sigmas()*self.der.sigmas()+
                          self.nat.sigmas()*self.nat.sigmas() )
      ).set_observation_type( self.der )



class outlier_rejection(object):
  def __init__(self,
               nat,
               der,
               cut_level_rms=3,
               cut_level_sigma=0,
               method={'solve':False,'rms':False, 'rms_and_sigma':True},
               out=None
               ):
    self.out=out
    self.method = method
    if self.out == None:
      self.out = sys.stdout

    self.cut_level_rms=cut_level_rms
    self.cut_level_sigma=cut_level_sigma
    ## Just make sure that we have the common sets
    self.nat = nat.deep_copy()
    self.der = der.deep_copy()
    self.nat, self.der = self.nat.common_sets(self.der)
    #Make sure we have amplitudes
    assert self.nat.is_real_array()
    assert self.nat.is_real_array()

    if self.nat.is_xray_intensity_array():
      self.nat=self.nat.f_sq_as_f()
    if self.der.is_xray_intensity_array():
      self.der=self.der.f_sq_as_f()

    ## Construct delta f's
    delta_gen = delta_generator( self.nat, self.der )
    self.delta_f = delta_gen.abs_delta_f
    ## Set up a binner please
    self.delta_f.setup_binner_d_star_sq_step(auto_binning=True)
    ## for each bin, I would like to compute
    ## mean dF**2
    ## mean sigma**2
    self.mean_df2 = self.delta_f.mean_of_intensity_divided_by_epsilon(
      use_binning=True,
      return_fail=1e12)
    self.mean_sdf2 = self.delta_f.mean_of_squared_sigma_divided_by_epsilon(
      use_binning=True,
      return_fail=1e12)

    self.result = flex.bool(  self.delta_f.indices().size(), True )

    self.detect_outliers()
    self.remove_outliers()

  def detect_outliers(self):
    count_true = 0
    if (self.method['solve']):
      count_true+=1
    if (self.method['rms']):
      count_true+=1
    if (self.method['rms_and_sigma']):
      count_true+=1


    if not count_true==1:
      raise Sorry("Outlier removal protocol not specified properly")

    if (self.method['solve']):
      self.detect_outliers_solve()
    if (self.method['rms']):
      self.detect_outliers_rms()
    if (self.method['rms_and_sigma']):
      self.detect_outliers_sigma()




  def detect_outliers_solve(self):
    """
    TT says:
    I toss everything > 3 sigma in the scaling,
    where sigma comes from the rms of everything being scaled:

    sigma**2 = <delta**2>- <experimental-sigmas**2>

    Then if a particular
    delta**2 > 3 sigma**2 + experimental-sigmas**2
    then I toss it.
    """
    terwilliger_sigma_array = flex.double(self.mean_df2.data) -\
                              flex.double(self.mean_sdf2.data)

    for bin_number in self.delta_f.binner().range_all():
      ## The selection tells us wether or not somthing is in the correct bin
      selection =  self.delta_f.binner().selection( bin_number ).iselection()
      ## Now just make a global check to test for outlierness:
      tmp_sigma_array =  terwilliger_sigma_array[bin_number] -\
                         self.delta_f.sigmas()*self.delta_f.sigmas()
      tmp_sigma_array = flex.sqrt(tmp_sigma_array)*self.cut_level_rms

      potential_outliers = ( self.delta_f.data()  >  tmp_sigma_array )
      potential_outliers =  potential_outliers.select( selection )

      self.result = self.result.set_selected( selection, potential_outliers )

    print >> self.out
    print >> self.out, " %8i potential outliers detected" %(
      self.result.count(True) )
    print >> self.out, " They will be removed from the data set"
    print >> self.out


  def detect_outliers_rms(self):
    for bin_number in self.delta_f.binner().range_all():
      selection =  self.delta_f.binner().selection( bin_number ).iselection()
      potential_outliers = (
        self.delta_f.data()  >  self.cut_level_rms*math.sqrt(
        self.mean_df2.data[bin_number])  )
      potential_outliers =  potential_outliers.select( selection )
      self.result = self.result.set_selected( selection, potential_outliers )

    print >> self.out
    print >> self.out, " %8i potential outliers detected" %(
      self.result.count(True) )
    print >> self.out, " They will be removed from the data set"
    print >> self.out


  def detect_outliers_sigma(self):
    ## Locate outliers in native
    potential_outlier_nat = (self.nat.data()/self.nat.sigmas()
                               < self.cut_level_sigma)
    nat_select = potential_outlier_nat.iselection()

    ## Locate outliers in derivative
    potential_outlier_der = (self.der.data()/self.der.sigmas()
                               <self.cut_level_sigma)
    der_select = potential_outlier_der.iselection()

    for bin_number in self.delta_f.binner().range_all():
      ## RMS outlier removal
      selection =  self.delta_f.binner().selection( bin_number ).iselection()
      potential_outliers = (
        self.delta_f.data()  >  self.cut_level_rms*math.sqrt(
        self.mean_df2.data[bin_number])  )
      potential_outliers =  potential_outliers.select( selection )
      self.result = self.result.set_selected( selection, potential_outliers )

    self.result = self.result.set_selected( nat_select, True )
    self.result = self.result.set_selected( der_select, True )

    print >> self.out
    print >> self.out, " %8i potential outliers detected" %(
      self.result.count(True) )
    print >> self.out, " They will be removed from the data set"
    print >> self.out


  def remove_outliers(self):
    potential_outliers = self.nat.select( self.result )

    matches = miller.match_indices( self.nat.indices(),
                                    potential_outliers.indices()  )

    self.nat = self.nat.select( matches.single_selection(0) )

    self.nat, self.der = self.nat.common_sets(self.der)


class f_double_prime_ratio(object):
  def __init__(self,
               lambda1,
               lambda2):
    ## make sure we have anomalous data
    assert ( lambda1.anomalous_flag() )
    assert ( lambda2.anomalous_flag() )
    ## make temporary copies
    tmp_l1 = lambda1.deep_copy()
    tmp_l2 = lambda2.deep_copy()
    ##make sure we have intensities
    assert tmp_l1.is_real_array()
    if tmp_l1.is_xray_amplitude_array():
      tmp_l1=tmp_l1.f_as_f_sq()

    assert tmp_l2.is_real_array()
    if tmp_l2.is_xray_amplitude_array():
      tmp_l2=tmp_l2.f_as_f_sq()

    ## we only need the anomalous diffs
    l1p, l1n = tmp_l1.hemispheres_acentrics()
    self.diff1 = l1p.data()-l1n.data()
    self.v1 = ( l1p.sigmas()*l1p.sigmas() +
                l1n.sigmas()*l1n.sigmas() )

    l2p, l2n = tmp_l2.hemispheres_acentrics()
    self.diff2 = l2p.data()-l2n.data()
    self.v2 = ( l2p.sigmas()*l2p.sigmas() +
                l2n.sigmas()*l2n.sigmas() )

    self.x=flex.double([1.0])
    scitbx.lbfgs.run(target_evaluator=self)
    self.ratio=self.x[0]
    #self.show()

  def compute_functional_and_gradients(self):
    f = self.compute_functional()
    g = self.compute_gradient()
    ##g2 =  self.compute_gradient_fd()
    return f,g

  def compute_functional(self):
    top = self.diff1 - self.diff2*self.x[0]
    top= top*top
    bottom = self.v1+self.v2*self.x[0]*self.x[0]
    result = top/bottom
    result=flex.sum(result)
    return result

  def compute_gradient(self):
    tmp_bottom = self.v1+self.v2*self.x[0]*self.x[0]
    tmp_top = self.diff1 - self.diff2*self.x[0]
    part1 = -2.0*self.x[0]*tmp_top*tmp_top*self.v2/( tmp_bottom*tmp_bottom )
    part2 = -2.0*self.diff2*tmp_top/tmp_bottom
    result=flex.sum( part1+part2)
    return(flex.double([result]))

  def compute_gradient_fd(self):
    h=0.0000001
    current = self.compute_functional()
    self.x[0]+=h
    new = self.compute_functional()
    self.x[0]-=h
    result = current - new
    result/=-h
    return flex.double([result])

  def show(self,out=None):
    if out is None:
      out = sys.stdout
    print >> out, "Inspired by: Kingston. Acta Cryst. (2001). D57, 101-107"
    print >> out
    print >> out, "Estimated ratio f\"(w1)/f\"(w2): %3.2f"%(self.x[0])
    print >> out



class delta_f_prime_f_double_prime_ratio(object):
  def __init__(self,
               lambda1,
               lambda2,
               out=None,
               level=1.0):
    self.out = out
    if self.out == None:
      self.out = sys.stdout
    
    self.tmp_l1 = lambda1.deep_copy()
    self.tmp_l2 = lambda2.deep_copy()

    ## make sure we have amplitudes please
    if self.tmp_l1.is_xray_intensity_array():
      self.tmp_l1 = self.tmp_l1.f_sq_as_f()
    if self.tmp_l2.is_xray_intensity_array():
      self.tmp_l2 = self.tmp_l2.f_sq_as_f()

    ## Now this is done, swe have to set up a binner
    self.tmp_l1, self.tmp_l2 = self.tmp_l1.common_sets( self.tmp_l2 )
    self.tmp_l1.setup_binner_d_star_sq_step( auto_binning=True )
    self.tmp_l2.use_binner_of( self.tmp_l1 )

    self.tmp_l1_no_ano = self.tmp_l1.average_bijvoet_mates()
    self.tmp_l2_no_ano = self.tmp_l2.average_bijvoet_mates()
    self.tmp_l1_no_ano.setup_binner_d_star_sq_step( auto_binning=True )
    self.tmp_l2_no_ano.setup_binner_d_star_sq_step( auto_binning=True )

    self.plus2, self.minus2 = self.tmp_l2.hemispheres_acentrics()
    self.plus2.use_binning_of( self.tmp_l1_no_ano )
    self.minus2.use_binning_of( self.tmp_l1_no_ano )

    ## we assume that the data is properly scaled of course
    ## Loop over all bins, and in each bin,
    ## compute <diso>, <df> and their ratio
    estimates = flex.double()
    count = 0
    for bin in self.tmp_l1.binner().range_all():
      selection =  self.tmp_l1_no_ano.binner().selection( bin ).iselection()
      tmp1 = self.tmp_l1_no_ano.select( selection ).data()
      tmp2 = self.tmp_l2_no_ano.select( selection ).data()
      stmp1 = self.tmp_l1_no_ano.select( selection ).sigmas()
      stmp2 = self.tmp_l2_no_ano.select( selection ).sigmas()

      selection = self.plus2.binner().selection( bin ).iselection()
      tmp3 = self.plus2.select( selection ).data()
      tmp4 = self.minus2.select( selection ).data()
      stmp3 = self.plus2.select( selection ).sigmas()
      stmp4 = self.minus2.select( selection ).sigmas()

      tmpiso=None
      tmpsiso=None
      tmpano=None
      tmpsano=None

      if tmp1.size() > 0:
        tmpiso = flex.mean( (tmp1 - tmp2)*(tmp1 - tmp2) )
        tmpsiso = flex.mean( stmp1*stmp1 +  stmp2*stmp2  )
      if tmp3.size() > 0:
        tmpano = flex.mean( (tmp3 - tmp4)*(tmp3 - tmp4) )
        tmpsano = flex.mean( stmp3*stmp3 +  stmp4*stmp4  )

      if tmp1.size() > 0: ## make sure something is there
        if tmp3.size() > 0: ## make sure something is there
          if math.sqrt(tmpiso/tmpsiso)>=level:  ## s/n is okai
            if math.sqrt(tmpano/tmpsano)>=level: ## s/n is okai
              delta_iso_sq = math.sqrt( tmpiso  )
              delta_ano_sq = math.sqrt( tmpano  )
              tmp=2.0*delta_iso_sq/delta_ano_sq
              estimates.append( tmp )
              count+=1.0

    ## compute the trimean please
    self.ratio = None
    self.number = count
    self.total = len(self.tmp_l1.binner().range_all())
    if (self.number>0):
      self.ratio=robust_statistics.trimean( estimates )

    #self.show()

  def show(self):
    if self.number > 0:
      print >> self.out
      print >> self.out, "Estimated ratio (|f'(w1)-f'(w2)|)/f\"(w2): %3.2f"\
            %(self.ratio)
      print >> self.out, "Number of resolution bins used : %3i" %( self.number )
      print >> self.out, "                                 (out of %3i)"\
            %(self.total)
    else:
      print >> self.out, "Unable to estimate the ratio (|f'(w1)-f'(w2)|)/f\"(w2)"

